<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">

    <title>Beneath Apache Spark's RDD API using spark-shell and webUI</title>

    <meta name="description" content="Spark Summit East: Beneath Apache Spark's RDD API with spark-shell and webUI">
    <meta name="author" content="Jacek Laskowski">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/beige.css" id="theme">

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="reveal">

      <div class="slides">
        <section class="intro" data-transition="zoom">
          <img style="background:none; border:none; box-shadow:none;" data-src="images/spark-logo.png">
          <h1>Beneath RDD</h1>
          <h1>in Apache Spark</h1>
          <h3>Using spark-shell and WebUI</h3>
          <h4><a href="https://blog.jaceklaskowski.pl">Jacek Laskowski</a> / <a href="https://twitter.com/jaceklaskowski">@jaceklaskowski</a> / <a href="https://github.com/jaceklaskowski">GitHub</a> / <a href="http://bit.ly/mastering-apache-spark">Mastering Apache Spark Notes</a></h4>
        </section>

        <section id="about-the-host">
          <p>
            <img width="20%" src="images/jacek_laskowski_20141201_512px.png" style="border: 0">
          </p>
          <ul>
            <li><b>Jacek Laskowski</b> is an independent consultant</li>
            <li>Contact me at <b>jacek@japila.pl</b> or <a href="https://twitter.com/jaceklaskowski">@JacekLaskowski</a></li>
            <li>Delivering Development Services | Consulting | Training</li>
            <li>Building and leading development teams</li>
            <li>Mostly <a href="http://spark.apache.org/">Apache Spark</a> and <a href="http://www.scala-lang.org/">Scala</a> these days</li>
            <li>Leader of <a href="http://warsawscala.pl">Warsaw Scala Enthusiasts</a> and <a href="http://www.meetup.com/Warsaw-Spark/">Warsaw Apache Spark</a></li>
            <li><a href="https://java.net/website/java-champions/bios.html#Laskowski">Java Champion</a></li>
            <li>Blogger at <a href="http://blog.jaceklaskowski.pl">blog.jaceklaskowski.pl</a> and <a href="http://jaceklaskowski.pl">jaceklaskowski.pl</a></li>
          </ul>
        </section>

        <section>
          <section id="sparkcontext">
            <h1>SparkContext</h1>
            <h3>The living space for RDDs</h3>
          </section>
          <section id="sparkcontext-intro">
            <h2>SparkContext and RDDs</h2>
            <ul>
              <li>
                An RDD belongs to <i><b>one and only one</b></i> Spark context.
                <ul>
                  <li>
                    You <i><b>cannot</b></i> share RDDs between contexts.
                  </li>
                </ul>
              </li>
              <li>
                SparkContext tracks how many RDDs were created.
                <ul>
                  <li>
                    You may see it in <code>toString</code> output.
                  </li>
                </ul>
              </li>
            </ul>
          </section>
        </section>

        <section>
          <section id="rdd">
            <h1>RDD</h1>
            <h3>Resilient Distributed Dataset</h3>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## Creating RDD - sc.parallelize

* **`sc.parallelize(col, slices)`** to distribute a local collection of any elements.

```
scala> val rdd = sc.parallelize(0 to 10)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at <console>:24
```

* <!-- li.element: class="fragment" --> Alternatively, **`sc.makeRDD(col, slices)`**
            </script>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## Creating RDD - sc.range

* **`sc.range(start, end, step, slices)`** to create RDD of long numbers.

```
scala> val rdd = sc.range(0, 100)
rdd: org.apache.spark.rdd.RDD[Long] = MapPartitionsRDD[14] at range at <console>:24
```
            </script>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## Creating RDD - sc.textFile

* **`sc.textFile(name, partitions)`** to create a RDD of lines from a file.

```
scala> val rdd = sc.textFile("README.md")
rdd: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[16] at textFile at <console>:24
```
            </script>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## Creating RDD - sc.wholeTextFiles

* **`sc.wholeTextFiles(name, partitions)`** to create a RDD of pairs of a file name and its content from a directory.

```
scala> val rdd = sc.wholeTextFiles("tags")
rdd: org.apache.spark.rdd.RDD[(String, String)] = tags MapPartitionsRDD[18] at wholeTextFiles at <console>:24
```
            </script>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
There are many more more advanced functions in `SparkContext` to create RDDs.
            </script>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## Partitions (and slices)

* Did you notice the words **slices** and **partitions** as parameters?
* <!-- li.element: class="fragment" --> **Partitions** (aka _slices_) are the **level of parallelism**.

<!-- .element: align="center" -->
_We're going to talk about the level of parallelism later._ <!-- li.element: class="fragment" -->
            </script>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## Creating RDD - DataFrames

* RDDs are so last year :-) Use DataFrames..._early and often!_
* <!-- .element: class="fragment" --> A **DataFrame** is a higher-level abstraction over RDDs and structured data.
* <!-- .element: class="fragment" --> DataFrames require a **SQLContext**.
            </script>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
## From RDDs to DataFrames

```
scala> val rdd = sc.parallelize(0 to 10)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[51] at parallelize at <console>:24

scala> val df = rdd.toDF
df: org.apache.spark.sql.DataFrame = [_1: int]

scala> val df = rdd.toDF("numbers")
df: org.apache.spark.sql.DataFrame = [numbers: int]
```
            </script>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
## ...And vice versa

```
scala> val rdd = sc.parallelize(0 to 10)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[51] at parallelize at <console>:24

scala> val df = rdd.toDF("numbers")
df: org.apache.spark.sql.DataFrame = [numbers: int]

scala> df.rdd
res23: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[70] at rdd at <console>:29
```
            </script>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## Creating DataFrames - sqlContext.createDataFrame

* **`sqlContext.createDataFrame(rowRDD, schema)`**
            </script>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## Creating DataFrames - sqlContext.read

* **`sqlContext.read`** is the modern yet _experimental_ way.
* **`sqlContext.read.format(f).load(path)`**, where `f` is:
 * jdbc
 * json
 * orc
 * parquet
 * text
            </script>
          </section>
        </section>

        <section>
          <section id="execution-environment">
            <h1>Execution Environment</h1>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## Partitions and Level of Parallelism

* The number of partitions of a RDD is (roughly) the number of tasks.
* Partitions are the hint to size jobs.
* **Tasks** are the smallest unit of execution.
* Tasks belong to **TaskSets**.
* TaskSets belong to **Stages**.
* Stages belong to **Jobs**.
* Jobs, stages, and tasks are displayed in **web UI**.

_We're going to talk about the web UI later._ <!-- li.element: class="fragment" -->
            </script>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## Partitions and Level of Parallelism cd.

* In **local[*]** mode, the number of partitions equals the number of cores (the default in `spark-shell`)

```
scala> sc.defaultParallelism
res0: Int = 8

scala> sc.master
res1: String = local[*]
```

* <!-- li.element: class="fragment" --> Not necessarily true when you use **local** or **local[n]** master URLs.
            </script>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## Level of Parallelism in Spark Clusters

* **TaskScheduler** controls the level of parallelism
* <!-- li.element: class="fragment" --> **DAGScheduler**, TaskScheduler, **SchedulerBackend** work in tandem
* <!-- li.element: class="fragment" --> **DAGScheduler** manages a "DAG" of RDDs (aka **RDD lineage**)
* <!-- li.element: class="fragment" --> **SchedulerBackends** manage TaskSets
            </script>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## RDD lineage

* **RDD lineage** is a graph of RDD dependencies.
* <!-- li.element: class="fragment" --> Use **toDebugString** to know the lineage.
* <!-- li.element: class="fragment" --> Be careful with the hops - they introduce **shuffle barriers**.
* <!-- li.element: class="fragment" --> Why is the RDD lineage important?
* <!-- li.element: class="fragment" --> This is the **R** in RDD - **resiliency**.
* <!-- li.element: class="fragment" --> But deep lineage costs processing time, _doesn't it?_
* <!-- li.element: class="fragment" --> **Persist** (aka **cache**) it early and often!
            </script>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## RDD lineage - demo

* What does the following do?

```
val rdd = sc.parallelize(0 to 10).map(n => (n % 2, n)).groupBy(_._1)
```

            </script>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## RDD lineage - demo cd.

* How many stages are there?

```
// val rdd = sc.parallelize(0 to 10).map(n => (n % 2, n)).groupBy(_._1)
scala> rdd.toDebugString
res2: String =
(2) ShuffledRDD[3] at groupBy at <console>:24 []
 +-(2) MapPartitionsRDD[2] at groupBy at <console>:24 []
    |  MapPartitionsRDD[1] at map at <console>:24 []
    |  ParallelCollectionRDD[0] at parallelize at <console>:24 []
```

* <!-- li.element: class="fragment" --> Nothing happens yet - processing time-wise.
            </script>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## Spark Clusters

* Spark supports the following clusters:
 * one-JVM local cluster
 * Spark Standalone
 * Apache Mesos
 * Hadoop YARN
* <!-- li.element: class="fragment" --> You use **--master** to select the cluster
 * <!-- li.element: class="fragment" --> **spark://hostname:port** is for Spark Standalone
* <!-- li.element: class="fragment" --> And you know the local master URL, _ain't you_?
 * <!-- li.element: class="fragment" --> **local**, **local[n]**, or **local[*]**
            </script>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## Mandatory Properties of Spark App

Your task: Fill in the gaps below.

**Any** Spark application **must** specify **a**<!-- .element: class="fragment" -->**pplication**<!-- .element: class="fragment" --> **n**<!-- .element: class="fragment" -->**ame** <!-- .element: class="fragment" --> (aka **appName** <!-- .element: class="fragment" -->) and **m**<!-- .element: class="fragment" -->**aster URL**. <!-- .element: class="fragment" -->
 *  <!-- li.element: class="fragment" --> _Demo time!_ => spark-shell is a Spark app, too!
            </script>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## Spark Standalone Cluster

* The built-in Spark cluster
* <!-- li.element: class="fragment" --> Start **standalone Master** with **sbin/start-master**
 * <!-- li.element: class="fragment" --> Use **-h** to control the host name to bind to.
* <!-- li.element: class="fragment" --> Start **standalone Worker** with **sbin/start-slave**
 * <!-- li.element: class="fragment" --> Run single worker per machine (aka **node**)
* <!-- li.element: class="fragment" --> http://localhost:8080/ = web UI for Standalone cluster
 * <!-- li.element: class="fragment" --> Don't confuse it with the web UI of Spark application
*  <!-- li.element: class="fragment" --> _Demo time!_ => Run Standalone cluster
            </script>
          </section>
        </section>

        <section>
          <section id="spark-shell">
            <h1>spark-shell</h1>
            <h3>Spark REPL Application</h3>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## spark-shell and Spark Standalone

* You can connect to Spark Standalone using **spark-shell** through **--master**<!-- .element: class="fragment" --> command-line option.
 * <!-- li.element: class="fragment" --> _Demo time!_ => we've already started the Standalone cluster.
            </script>
          </section>
        </section>

        <section>
          <section id="webui">
            <h1>webUI</h1>
            <h3>Web User Interface for Spark Application</h3>
          </section>
          <section data-transition="zoom" data-markdown>
            <script type="text/template">
            <!-- .slide: align="left" -->
## webUI

* It is available under http://localhost:4040/
* <!-- li.element: class="fragment" --> You can disable it using **spark.ui.enabled** flag.
* <!-- li.element: class="fragment" --> All the events are captured by **Spark listeners**
 * <!-- li.element: class="fragment" --> You can register your own Spark listener.
* <!-- li.element: class="fragment" --> _Demo time!_ => webUI in action with different master URLs
            </script>
          </section>
        </section>

        <section id="thats-all" data-background-image="images/end.jpg">
          <!-- THAT'S IT FOLKS -->
        </section>

        <section id="questions" style="text-align: left">
          <h1>Questions?</h1>
          <p>
            - Visit <a href="https://blog.jaceklaskowski.pl">Jacek Laskowski's blog</a><br>
            - Follow <a href="https://twitter.com/jaceklaskowski">@jaceklaskowski</a> at twitter<br>
            - Use <a href="https://github.com/jaceklaskowski">Jacek's projects at GitHub</a><br>
            - Read <a href="http://bit.ly/mastering-apache-spark">Mastering Apache Spark</a> notes.
          </p>
        </section>

      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>

        // Full list of configuration options available at:
        // https://github.com/hakimel/reveal.js#configuration
        Reveal.initialize({
            controls: true,
            progress: true,
            history: true,
            center: true,

            transition: 'slide', // none/fade/slide/convex/concave/zoom

            // Optional reveal.js plugins
            dependencies: [
                { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
                { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
                { src: 'plugin/zoom-js/zoom.js', async: true },
                { src: 'plugin/notes/notes.js', async: true }
            ]
        });

    </script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-45999426-3', 'auto');
      ga('send', 'pageview');

    </script>
  </body>
</html>
